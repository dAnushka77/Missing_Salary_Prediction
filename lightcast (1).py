# -*- coding: utf-8 -*-
"""Lightcast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14jO64lmTXKxr9oCPYetc-5pYz_-rleg6
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install packages (run this separately first)
!pip install lightgbm fairlearn --quiet

# Imports
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score, confusion_matrix, ConfusionMatrixDisplay,
    mean_absolute_error, mean_squared_error, r2_score
)
from sklearn.feature_extraction.text import TfidfVectorizer
from lightgbm import LGBMRegressor
from fairlearn.metrics import MetricFrame, mean_prediction
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
file_path = '/content/drive/MyDrive/glassdoor_jobs.csv'
df = pd.read_csv(file_path)
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

# Split into known and missing salaries
df_known = df[df['salary_estimate'] != '-1'].copy()
df_missing = df[df['salary_estimate'] == '-1'].copy()

df.shape

# Check if there are any duplicate rows
has_duplicates = df.duplicated().any()

# Print result
if has_duplicates:
    print("Duplicates found in the DataFrame.")
    # Show the duplicated rows
    print(df[df.duplicated()])
else:
    print("No duplicate rows in the DataFrame.")

if df.isnull().values.any():
    print("Missing values detected in the DataFrame.")
else:
    print("No missing values found.")



df_missing.head()

df_missing_cleaned = df_missing.drop(columns=['unnamed:_0', 'competitors', 'revenue'], errors='ignore').reset_index(drop=True)
df_missing_cleaned.head()

df_known.head()

# Extract salary ranges
df_known['salary_range'] = df_known['salary_estimate'].apply(lambda x: re.findall(r'\$?(\d+)[kK]?', x))
df_known['min_salary'] = df_known['salary_range'].apply(lambda x: int(x[0])*1000 if len(x) > 0 else np.nan)
df_known['max_salary'] = df_known['salary_range'].apply(lambda x: int(x[1])*1000 if len(x) > 1 else np.nan)
df_known['avg_salary'] = df_known[['min_salary', 'max_salary']].mean(axis=1)

# Optional: Remove outliers using IQR on avg_salary
Q1 = df_known['avg_salary'].quantile(0.25)
Q3 = df_known['avg_salary'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

df_known = df_known[(df_known['avg_salary'] >= lower_bound) & (df_known['avg_salary'] <= upper_bound)]

# Combine
df_combined = pd.concat([df_known, df_missing], ignore_index=True)
df_combined['was_missing'] = df_combined['salary_estimate'] == '-1'
df_combined.head()

# Feature Engineering
df_combined['seniority'] = df_combined['job_title'].str.extract(r'(junior|mid|senior|lead|principal)', flags=re.IGNORECASE)
df_combined['seniority'] = df_combined['seniority'].fillna('unknown').str.lower()
#df_combined[['job_city', 'job_state']] = df_combined['location'].str.extract(r'^(.*),\s*([A-Z]{2})$')
df_combined['posting_age'] = df_combined['job_description'].str.extract(r'(\d+)\+?\s*(?=day|month|week|year)', flags=re.IGNORECASE)
df_combined['posting_age'] = pd.to_numeric(df_combined['posting_age'][0], errors='coerce')
df_combined['posting_age'] = df_combined['posting_age'].fillna(df_combined['posting_age'].median())


# Extract city and state from location if format is "City, ST"
df_combined[['job_city', 'job_state']] = df_combined['location'].str.extract(r'^(.*),\s*([A-Z]{2})$')

# Fill job_state from single-word locations if job_state is still missing
df_combined['job_state'] = df_combined['job_state'].fillna(
    df_combined['location'].where(~df_combined['location'].str.contains(','), np.nan)
)

# Same for job_city if needed
df_combined['job_city'] = df_combined['job_city'].fillna(
    df_combined['location'].where(~df_combined['location'].str.contains(','), np.nan)
)

# TF-IDF on job_description
tfidf = TfidfVectorizer(max_features=50, stop_words='english')
tfidf_matrix = tfidf.fit_transform(df_combined['job_description'].fillna("")).toarray()
tfidf_df = pd.DataFrame(tfidf_matrix, columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])])
df_combined = pd.concat([df_combined.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)

# Prepare for classification
df_combined['missing_salary'] = df_combined['avg_salary'].isnull().astype(int)
seniority_map = {'unknown': 0, 'junior': 1, 'mid': 2, 'senior': 3, 'lead': 4, 'principal': 5}
df_combined['seniority_level'] = df_combined['seniority'].map(seniority_map).fillna(0)

X_stage1 = pd.get_dummies(df_combined[['job_title', 'company_name', 'job_state', 'seniority']].fillna("missing"))
y_stage1 = df_combined['missing_salary']
X_train, X_test, y_train, y_test = train_test_split(X_stage1, y_stage1, test_size=0.1, random_state=42)

# Classification models
clf_log = LogisticRegression(max_iter=1000).fit(X_train, y_train)
clf_rf = RandomForestClassifier(random_state=42).fit(X_train, y_train)
clf_knn = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)

# Accuracy
print("\nModel Classification Accuracy")
print("Logistic Regression: $ {:.3f}".format(accuracy_score(y_test, clf_log.predict(X_test))))
print("RandomForest Classifier: $ {:.3f}".format(accuracy_score(y_test, clf_rf.predict(X_test))))
print("KNN: $ {:.3f}".format(accuracy_score(y_test, clf_knn.predict(X_test))))

# Confusion Matrices
models = {"Logistic Regression": clf_log, "Random Forest": clf_rf, "KNN": clf_knn}
fig, axes = plt.subplots(1, 3, figsize=(12, 3))
for ax, (name, model) in zip(axes, models.items()):
    cm = confusion_matrix(y_test, model.predict(X_test))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(ax=ax, cmap="Blues", colorbar=False)
    ax.set_title(name)
plt.tight_layout()
plt.show()

# Regression for known salaries
features = ['job_title', 'company_name', 'industry', 'seniority', 'job_state']
df_train_reg = df_combined[df_combined['avg_salary'].notnull()].copy()
X_rf = pd.get_dummies(df_train_reg[features].fillna("missing"))
X_rf = pd.concat([X_rf, df_train_reg[[c for c in df_combined.columns if c.startswith("tfidf_")]]], axis=1)
y_rf = df_train_reg['avg_salary']
reg_rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_rf, y_rf)

# Predict for missing
df_missing_rows = df_combined[df_combined['avg_salary'].isnull()].copy()
X_missing_rf = pd.get_dummies(df_missing_rows[features].fillna("missing"))
X_missing_rf = pd.concat([X_missing_rf, df_missing_rows[[c for c in df_combined.columns if c.startswith("tfidf_")]]], axis=1)
X_missing_rf = X_missing_rf.reindex(columns=X_rf.columns, fill_value=0)

predicted_salaries = reg_rf.predict(X_missing_rf)

# Save to dataframe
df_combined.loc[df_combined['was_missing'], 'avg_salary_predicted'] = predicted_salaries
df_combined['final_salary'] = df_combined['avg_salary']
df_combined.loc[df_combined['final_salary'].isnull(), 'final_salary'] = df_combined['avg_salary_predicted']

# Evaluation for regressor
print("\nRandom Forest Regressor Metrics:")
print("MAE: $ {:.3f}".format(mean_absolute_error(y_rf, reg_rf.predict(X_rf))))
print("RMSE: $ {:.3f}".format(mean_squared_error(y_rf, reg_rf.predict(X_rf)) ** 0.5))
print("R² Score: $ {:.3f}".format(r2_score(y_rf, reg_rf.predict(X_rf))))

# LightGBM + Monotonic
features_lgbm = ['seniority_level', 'posting_age', 'job_state', 'industry', 'company_name', 'job_title']
df_lgbm = df_combined[df_combined['avg_salary'].notnull()].copy()
X_lgbm = pd.get_dummies(df_lgbm[features_lgbm].fillna("missing"))
X_lgbm['seniority_level'] = df_lgbm['seniority_level']
y_lgbm = df_lgbm['avg_salary']

# Sanitize column names
def sanitize_and_dedup_columns(df):
    df.columns = df.columns.str.replace(r'[^A-Za-z0-9_]+', '_', regex=True)
    seen = {}
    new_cols = []
    for col in df.columns:
        if col not in seen:
            seen[col] = 1
            new_cols.append(col)
        else:
            seen[col] += 1
            new_cols.append(f"{col}_{seen[col]}")
    df.columns = new_cols
    return df

X_lgbm = sanitize_and_dedup_columns(X_lgbm)
X_train_lgbm, X_test_lgbm, y_train_lgbm, y_test_lgbm = train_test_split(X_lgbm, y_lgbm, test_size=0.1, random_state=42)
monotonic_constraints = [1 if col == 'seniority_level' else 0 for col in X_train_lgbm.columns]

model_lgbm = LGBMRegressor(monotone_constraints=monotonic_constraints, n_estimators=100, random_state=42)
model_lgbm.fit(X_train_lgbm, y_train_lgbm)
y_pred_lgbm = model_lgbm.predict(X_test_lgbm)

# Evaluation
print("\nLightGBM Monotonic Metrics:")
print("MAE: $ {:.3f}".format(mean_absolute_error(y_test_lgbm, y_pred_lgbm)))
print("RMSE: $ {:.3f}".format(mean_squared_error(y_test_lgbm, y_pred_lgbm) ** 0.5))
print("R²: {:.3f}".format(r2_score(y_test_lgbm, y_pred_lgbm)))

# Fairness audit
df_combined.loc[X_test_lgbm.index, 'lgbm_predicted'] = y_pred_lgbm
grouping_feature = df_combined.loc[X_test_lgbm.index, 'job_state']
mf = MetricFrame(metrics=mean_prediction,
                 y_true=df_combined.loc[X_test_lgbm.index, 'avg_salary'],
                 y_pred=df_combined.loc[X_test_lgbm.index, 'lgbm_predicted'],
                 sensitive_features=grouping_feature)

print("\nFairness Audit by State:")
print(mf.by_group.sort_values(ascending=False))
mf.by_group.sort_values().plot(kind='barh', figsize=(7, 4), title="Avg Predicted Salary by State")
plt.xlabel("Salary")
plt.tight_layout()
plt.show()

df_final = df_combined[[
    'job_title', 'salary_estimate','company_name', 'industry', 'sector', 'job_city', 'job_state',  'size',	'founded',	'type_of_ownership','revenue',	'seniority',
    'seniority_level','avg_salary', 'avg_salary_predicted',  'final_salary', 'was_missing'
]]
df_final.tail()

# Save df_selected to CSV
df_final.to_csv("df_final_output.csv", index=False)

# Download the file
from google.colab import files
files.download("df_final_output.csv")

df_final['was_missing'].value_counts()



# Create final_salary from available or predicted
#df_combined['final_salary'] = df_combined['avg_salary']
#df_combined.loc[df_combined['was_missing'], 'final_salary'] = df_combined.loc[df_combined['was_missing'], 'avg_salary_predicted']

# Just the rows where salary_estimate was originally missing
df_predicted_only = df_combined[df_combined['was_missing'] == True][[
    'job_title', 'company_name', 'salary_estimate' ,'final_salary'
]].reset_index(drop=True)

print("\nPredicted Missing Salary Rows:")
df_predicted_only.tail()



# Just the rows where salary_estimate was originally missing
df_predicted_only = df_combined[df_combined['was_missing'] == False][[
    'job_title', 'company_name', 'salary_estimate', 'final_salary'
]].reset_index(drop=True)

print("\nAvailable Salary Rows:")
df_predicted_only.head()





# Filter rows where salary was missing and is now predicted
df_present_screenshot = df_combined[df_combined['was_missing'] == 1][[
    'job_title',
    'company_name',
    'industry',
    'job_state',
    'job_city',
    'seniority_level',
    'salary_estimate',
    'final_salary',
    'was_missing'
]].reset_index(drop=True)

df_present_screenshot.head()  # or .to_csv() for export

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=df_present_screenshot)





# Add avg_salary_predicted to show what the model predicted
df_present_technical = df_combined[df_combined['was_missing'] == 1][[
    'job_title',
    'company_name',
    'industry',
    'job_state',
    'salary_estimate',
    'avg_salary_predicted',
    'final_salary',
    'was_missing'
]].reset_index(drop=True)

df_present_technical.head()



